#!/usr/bin/python'''###FINAL RESULTS###KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',           metric_params=None, n_neighbors=5, p=2, weights='distance')        Accuracy: 0.86969       Precision: 0.63235      Recall: 0.36550 F1: 0.46324     F2: 0.39919        Total predictions: 13000        True positives:  731    False positives:  425   False negatives: 1269   True negatives: 10575'''import sysimport pickle#sys.path.append("../tools/")#All files are in the final_project folderimport osos.getcwd()os.chdir("/Users/jas/Project-4-Identifying-Fraud-from-Enron-Email/final_project")os.getcwd()from feature_format import featureFormat, targetFeatureSplitfrom tester import test_classifier, dump_classifier_and_data### Task 1: Select what features you'll use.### features_list is a list of strings, each of which is a feature name.### The first feature must be "poi".features_list = ['poi','salary','exercised_stock_options','bonus','expenses',				'loan_advances','from_this_person_to_poi',				'from_poi_to_this_person'] # You will need to use more featuresfeatures_list = ['poi','salary','exercised_stock_options','bonus','expenses',				'loan_advances'] # You will need to use more featuresfeatures_list = ['poi','salary', 'bonus'] # You will need to use more features#Results if I only used salary and bonus'''KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',           metric_params=None, n_neighbors=3, p=2, weights='uniform')        Accuracy: 0.80900       Precision: 0.54032      Recall: 0.30150 F1: 0.38703     F2: 0.33074        Total predictions: 10000        True positives:  603    False positives:  513   False negatives: 1397   True negatives: 7487'''###FINAL CHOSEN FEATURES###features_list = ['poi','salary','exercised_stock_options', 'bonus']#This simpler model gives higher recall and precision than more features and#accuracy only goes down slightly - less than 1 %### Load the dictionary containing the datasetdata_dict = pickle.load(open("final_project_dataset.pkl", "r") )###All employee names in this datasetfor employee in data_dict:    print employeefor employee in data_dict:    print data_dict[employee]### Print one exampledata_dict['LAY KENNETH L']### Task 2: Remove outliers'''Usual Steps:  1. Train - DONE 2. Remove points with largest residual error (10%) ? 3. Train again, repeat if necessary   There are no residual errors for kNN classification (predicted result is either 0 or 1 (POI or Non-POI)), the example given in video seems more appropriate for regression problems.'''#Let's try a simpler approach by looking for outliers by inspecting the dictionary.#Look at one employee onlydata_dict['LAY KENNETH L']#Let's see all employee namesfor employee in data_dict:    print employee#What is the Travel Agency & Total?data_dict['THE TRAVEL AGENCY IN THE PARK'] #mostly NaNs not a persondata_dict['TOTAL'] #just the Total#Let's see if there are any duplicatesemployee_names = []for employee in data_dict:    employee_names.append(employee)len(employee_names)employee_set = set(employee_names)len(employee_set)    #No duplicates...#Maybe there are names spelled slightly different twice?#Sort alphabetically and visually inspectemployee_names.sort()employee_names #using iPython to see output#All names except for Total and Travel Agency seem valid###Remove the 2 outliers from the original datasetdata_dict.pop('THE TRAVEL AGENCY IN THE PARK', 0)data_dict.pop('TOTAL', 0)len(data_dict) #144 records as expected#Before outlier removalKNeighborsClassifier(algorithm='kd_tree', leaf_size=30, metric='minkowski',           metric_params=None, n_neighbors=5, p=2, weights='uniform')        Accuracy: 0.87493       Precision: 0.62955      Recall: 0.30250 F1: 0.40865     F2: 0.33757        Total predictions: 14000        True positives:  605    False positives:  356   False negatives: 1395   True negatives: 11644#After outlier removalKNeighborsClassifier(algorithm='kd_tree', leaf_size=30, metric='minkowski',           metric_params=None, n_neighbors=5, p=2, weights='uniform')        Accuracy: 0.87907       Precision: 0.68472      Recall: 0.28450 F1: 0.40198     F2: 0.32216        Total predictions: 14000        True positives:  569    False positives:  262   False negatives: 1431   True negatives: 11738#Accuracy improved slightly, precision increased as well, but recall went down.### Task 3: Create new feature(s)#POIsfor employee in data_dict:    if data_dict[employee]['poi']:        print float(data_dict[employee]['restricted_stock'])/float(data_dict[employee]['total_stock_value'])        #print float(data_dict[employee]['expenses'])/float(data_dict[employee]['salary'])#Non-POIsfor employee in data_dict:    if not data_dict[employee]['poi']:        print float(data_dict[employee]['restricted_stock'])/float(data_dict[employee]['total_stock_value'])        print float(data_dict[employee]['expenses'])/float(data_dict[employee]['salary'])#About a third of POIs have all their stock as restricted_stock (1.0)#Let's add it to the data_dictfor employee in data_dict:    data_dict[employee]['restricted_stock_ratio'] = round(float(data_dict[employee]['restricted_stock']) / \                                                    float(data_dict[employee]['total_stock_value']),2)#Let's see how many values are NANcount_NAN = 0import mathfor employee in data_dict:        print data_dict[employee]['restricted_stock_ratio']    if math.isnan(data_dict[employee]['restricted_stock_ratio']):        count_NAN += 1percent_NAN = 100*float(count_NAN)/float(len(data_dict)) print percent_NAN # Over a quarter of all records are NA probably not good for feature### Store to my_dataset for easy export below.my_dataset = data_dict### Extract features and labels from dataset for local testingdata = featureFormat(my_dataset, features_list, sort_keys = True)labels, features = targetFeatureSplit(data)### Task 4: Try a variety of classifiers### Please name your classifier clf for easy export below.### Note that if you want to do PCA or other multi-stage operations,### you'll need to use Pipelines. For more info:### http://scikit-learn.org/stable/modules/pipeline.htmlfrom sklearn.naive_bayes import GaussianNBclf = GaussianNB()    # Provided to give you a starting point. Try a varity of classifiers.from sklearn import treeclf = tree.DecisionTreeClassifier()from sklearn.neighbors import KNeighborsClassifierclf = KNeighborsClassifier(n_neighbors=3)from sklearn.ensemble import RandomForestClassifierclf = RandomForestClassifier(n_estimators=10)from sklearn.ensemble import BaggingClassifierfrom sklearn.neighbors import KNeighborsClassifierclf = BaggingClassifier(KNeighborsClassifier(),                            max_samples=0.5, max_features=0.5)#This onefrom sklearn.neighbors import KNeighborsClassifierclf = KNeighborsClassifier(algorithm = "auto",n_neighbors=3,                           weights='uniform')###FINAL CHOSEN ALGORITHM AND PARAMETERS - Better Recall, more True Positivesfrom sklearn.neighbors import KNeighborsClassifierclf = KNeighborsClassifier(n_neighbors=5,weights='distance')####from sklearn.neighbors import KNeighborsClassifierclf = KNeighborsClassifier(algorithm = "kd_tree", leaf_size = 15,                           n_neighbors=5, weights='uniform')### Task 5: Tune your classifier to achieve better than .3 precision and recall ### using our testing script.### Because of the small size of the dataset, the script uses stratified### shuffle split cross validation. For more info: ### http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.StratifiedShuffleSplit.htmltest_classifier(clf, my_dataset, features_list)#features initially used for all algorithmsfeatures_list = ['poi','salary','exercised_stock_options','bonus','expenses',				'loan_advances','from_this_person_to_poi',				'from_poi_to_this_person'] #Other algorithms tried    DecisionTreeClassifier(compute_importances=None, criterion='gini',            max_depth=None, max_features=None, max_leaf_nodes=Nomne,            min_density=None, min_samples_leaf=1, min_samples_split=2,            random_state=None, splitter='best')Accuracy: 0.80707       Precision: 0.33521      Recall: 0.35650 F1: 0.34553     F2: 0.35203Total predictions: 14000        True positives:  713    False positives: 1414   False negatives: 1287   True negatives: 10586RandomForestClassifier(bootstrap=True, compute_importances=None,            criterion='gini', max_depth=None, max_features='auto',            max_leaf_nodes=None, min_density=None, min_samples_leaf=1,            min_samples_split=2, n_estimators=10, n_jobs=1,            oob_score=False, random_state=None, verbose=0)Accuracy: 0.84100       Precision: 0.34647      Recall: 0.12750 F1: 0.18640     F2: 0.14595Total predictions: 14000        True positives:  255    False positives:  481   False negatives: 1745   True negatives: 11519 ### Dump your classifier, dataset, and features_list so ### anyone can run/check your results.dump_classifier_and_data(clf, my_dataset, features_list)git add .