#!/usr/bin/python'''###FINAL RESULTS###KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',           metric_params=None, n_neighbors=5, p=2, weights='distance')        Accuracy: 0.86969       Precision: 0.63235      Recall: 0.36550 F1: 0.46324     F2: 0.39919        Total predictions: 13000        True positives:  731    False positives:  425   False negatives: 1269   True negatives: 10575'''import sysimport pickleimport pprint#sys.path.append("../tools/")#All files are in the final_project folder#### Comment out the 4 lines below before runningimport osos.getcwd()os.chdir("/Users/jas/Project-4-Identifying-Fraud-from-Enron-Email/final_project")os.getcwd()####from feature_format import featureFormat, targetFeatureSplitfrom tester import test_classifier, dump_classifier_and_data### Task 1: Select what features you'll use.### features_list is a list of strings, each of which is a feature name.### The first feature must be "poi".features_list = ['poi','salary','exercised_stock_options', 'bonus']#This simpler model gives higher recall and precision than more features and#accuracy only goes down slightly - less than 1 %### Load the dictionary containing the datasetdata_dict = pickle.load(open("final_project_dataset.pkl", "r") )### Task 2: Remove outliers'''Usual Steps:  1. Train - DONE 2. Remove points with largest residual error (10%) ? 3. Train again, repeat if necessary   There are no residual errors for kNN classification (predicted result is either 0 or 1 (POI or Non-POI)), the example given in video seems more appropriate for regression problems.'''#Let's try a simpler approach by looking for outliers by inspecting the dictionary.###All employee names in this datasetfor employee in data_dict:    print employeefor employee in data_dict:    pprint.pprint(data_dict[employee])### Print one examplepprint.pprint(data_dict['LAY KENNETH L'])#What is the Travel Agency & Total?pprint.pprint(data_dict['THE TRAVEL AGENCY IN THE PARK']) #mostly NaNs not a personpprint.pprint(data_dict['TOTAL']) #just the Total#Let's see if there are any duplicatesemployee_names = []for employee in data_dict:    employee_names.append(employee)print len(employee_names)employee_set = set(employee_names)print len(employee_set)    #No duplicates...#Maybe there are names spelled slightly different twice?#Sort alphabetically and visually inspectemployee_names.sort()pprint.pprint(employee_names) #All names except for Total and Travel Agency seem valid###Remove the 2 outliers from the original datasetdata_dict.pop('THE TRAVEL AGENCY IN THE PARK', 0)data_dict.pop('TOTAL', 0)print len(data_dict) #144 records as expected### Task 3: Create new feature(s)#POIsfor employee in data_dict:    if data_dict[employee]['poi']:        print float(data_dict[employee]['restricted_stock'])/float(data_dict[employee]['total_stock_value'])#Non-POIsfor employee in data_dict:    if not data_dict[employee]['poi']:        print float(data_dict[employee]['restricted_stock'])/float(data_dict[employee]['total_stock_value'])        print float(data_dict[employee]['expenses'])/float(data_dict[employee]['salary'])#About a third of POIs have all their stock as restricted_stock (1.0)#Let's add it to the data_dictfor employee in data_dict:    data_dict[employee]['restricted_stock_ratio'] = round(float(data_dict[employee]['restricted_stock']) / \                                                    float(data_dict[employee]['total_stock_value']),2)#Let's see how many values are NaNdef NaN_counter(feature_name):    "Calculates the percentage of NaNs in a feature"    count_NaN = 0    import math    for employee in data_dict:            if math.isnan(float(data_dict[employee][feature_name])):            count_NaN += 1    percent_NaN = 100*float(count_NaN)/float(len(data_dict))     percent_NaN = round(percent_NaN,2)    return percent_NaN    print str(NaN_counter('restricted_stock_ratio')) + " percent of values are NaN"    # Over a quarter of all records are NA probably not good for feature. I won't add it.###Can my model benefit from feature_scaling? Let's see. #from sklearn.preprocessing import MinMaxScaler#import numpy##Values contain NqN so need to do some imputation ##Let's check percentage of NqNs#print str(NaN_counter('salary')) + " percent of values are NaN" #34.72 %#print str(NaN_counter('exercised_stock_options')) + " percent of values are NaN" #29.86 %#print str(NaN_counter('bonus')) + " percent of values are NaN" #43.75 %###Let's do feature scaling on exercised_stock_options. Let's impute the most common value for NaNs.##exercised_stock_options_with_NaN = []#for employee in data_dict:#    exercised_stock_options_with_NaN.append(float(data_dict[employee]['salary']))##    options.append(float(data_dict[employee]['exercised_stock_options']))##    bonuses.append(float(data_dict[employee]['bonus']))##from collections import Counter#data = Counter(exercised_stock_options_with_NaN)#data.most_common(1)###Nope, most common is not the best strategy for imputation since almost all values are unique.###Let's look at the average and the median#numpy.mean(exercised_stock_options_with_NaN) #Nan#numpy.median(exercised_stock_options_with_NaN) #315915, let's use the median##exercised_stock_options_before_scaling = []#import math#for value in exercised_stock_options_with_NaN:#    print value    #    #if the value is a NaN, set it to the median value#    if math.isnan(float(value)):#        value = numpy.median(exercised_stock_options_with_NaN)#    exercised_stock_options_before_scaling.append(value)#print exercised_stock_options_before_scaling##exercised_stock_options_before_scaling = numpy.array(exercised_stock_options_before_scaling) #scaler = MinMaxScaler()#rescaled_exercised_stock_options = scaler.fit_transform(exercised_stock_options_before_scaling)###Let's add this scaled feature to the original dataset. #We haven't changed order of arrays.#for value in rescaled_exercised_stock_options:#    for employee in data_dict:#        data_dict[employee]['rescaled_exercised_stock_options'] = value#####Using feature scaling, I got a lower Accuracy, Precision, and Recall. My hypothesis is that####because a large percent of the data was imputed with the mean the feature scaling didn't help.#KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',#           metric_params=None, n_neighbors=5, p=2, weights='distance')#        Accuracy: 0.86133       Precision: 0.45943      Recall: 0.22650 F1: 0.30342     F2: 0.25206#        Total predictions: 15000        True positives:  453    False positives:  533   False negatives: 1547   True negatives: 12467### Store to my_dataset for easy export below.my_dataset = data_dict### Extract features and labels from dataset for local testingdata = featureFormat(my_dataset, features_list, sort_keys = True)labels, features = targetFeatureSplit(data)### Task 4: Try a variety of classifiers### Please name your classifier clf for easy export below.### Note that if you want to do PCA or other multi-stage operations,### you'll need to use Pipelines. For more info:### http://scikit-learn.org/stable/modules/pipeline.html###FINAL CHOSEN ALGORITHM AND PARAMETERS - Better Recall, more True Positivesfrom sklearn.neighbors import KNeighborsClassifierclf = KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',           metric_params=None, n_neighbors=5, p=2, weights='distance')### Task 5: Tune your classifier to achieve better than .3 precision and recall ### using our testing script.### Because of the small size of the dataset, the script uses stratified### shuffle split cross validation. For more info: ### http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.StratifiedShuffleSplit.htmltest_classifier(clf, my_dataset, features_list) ### Dump your classifier, dataset, and features_list so ### anyone can run/check your results.dump_classifier_and_data(clf, my_dataset, features_list)